In order to see my work, navigate to the folder spiders. (crawlerDemo/crawlerDemo/spiders)

Link to Slides from presentation: https://docs.google.com/presentation/d/13hzmejLQMmvbevf_C0iog1ui2Grvk2Q4A2xoTCCBmZ4/edit?usp=sharing

Resources used:

https://www.youtube.com/watch?v=m_3gjHGxIJc

https://www.datacamp.com/tutorial/making-web-crawlers-scrapy-python

https://docs.scrapy.org/en/latest/index.html

https://oxylabs.io/blog/how-to-crawl-a-website-without-getting-blocked

https://stackoverflow.com/questions/29883132/stop-scrapy-crawling-the-same-urls

https://stackoverflow.com/questions/29952353/cant-read-in-a-file-and-then-write-results-to-a-file-with-scrapy-python

https://www.w3schools.com/python/ref_file_readlines.asp

https://stackoverflow.com/questions/45967157/scrape-crawlspider-attributeerror-rules

https://stackoverflow.com/questions/61895896/what-are-the-risks-of-overriding-a-scrapy-spiders-init-method

https://stackoverflow.com/questions/27865334/iterate-scrapy-crawl-over-several-text-files-containing-urls

https://stackoverflow.com/questions/27509489/how-to-dynamically-set-scrapy-rules

https://www.youtube.com/watch?v=ph90tBmiask

https://www.codecademy.com/resources/docs/python/regex/sub
